\chapter{Conclusion}\label{Chap:4}

Experiments in Section~\ref{Sec: imp} demonstrate that TrueSkill, built upon Approximate Bayesian Inference, is a feasible solution to the Kaggle challenge. In fact, Model 1 from Table~\ref{Ta:results}, the na\"{i}ve TrueSkill model, was also used to participate in this year's similar Kaggle competition ``March Machine Learning Mania \emph{2017}''. It scored 0.481061 based on the same LogLoss evaluation method and ranked \nth{48} on the leaderboard~\cite{KG17}. 

Other models had not yet been built while the 2017 competition was accepting submissions, so they were instead tested on a \emph{simulated} 2015 competition, given that the \emph{real} 2015 competition was no longer accepting submissions. The 2015 NCAA Tournament results are provided in Kaggle's 2016 data sets and used as ground truths for the simulated competition. Models were allowed to use match records during the regular seasons up to 2015 to predict the outcomes of the 2015 tournament. The best model scored 0.479189 in this test and would have been ranked \nth{76} on the 2015's leaderboard. 

As mentioned in Section~\ref{SSec: features}, the most challenging part of this project is feature selection. The large number of possible features create exponentially many possible models. In this project, features are selected by human data scientists conducting tests where each feature is allowed to vary on a small scale. However, as a possible improvement, \emph{reinforcement learning} could be used to select features automatically in the future. Many simulated competitions for different years could be constructed for the machine-learning agent to select a best set of features to use. The LogLoss score can be used as rewards or punishments to guide the agent's decisions. Eventually, the agent should be able to obtain a set of optimal features that minimise the overall LogLoss score in the long run and act as if it were the perfect knowledge predictor. 